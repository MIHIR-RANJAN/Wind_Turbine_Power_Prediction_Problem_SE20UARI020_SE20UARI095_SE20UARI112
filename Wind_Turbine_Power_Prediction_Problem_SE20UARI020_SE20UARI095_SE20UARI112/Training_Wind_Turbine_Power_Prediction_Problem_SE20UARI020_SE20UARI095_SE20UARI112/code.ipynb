{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "P5qlmzkc2kt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install pandas scikit-learn tensorflow\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qTBl90U35R3",
        "outputId": "7f63512d-0c69-4343-8575-ba32b3eca19c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training validation and testing\n"
      ],
      "metadata": {
        "id": "879ee-OIlZ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('/content/dataset.csv')\n",
        "\n",
        "# Extract features and target variable\n",
        "X = dataset[['Wind Speed (m/s)', 'Theoretical_Power_Curve (KWh)', 'Wind Direction (Â°)']]\n",
        "y = dataset['LV ActivePower (kW)']\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into train, validate, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_normalized, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1), kernel_regularizer=l1(0.01)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Reshape data for LSTM (add time dimension)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Assuming 'y_test_last_20_pred' is a 2D array\n",
        "# Inverse transform the normalized predictions to get actual ActivePower values\n",
        "scaler = MinMaxScaler()\n",
        "y = y.values.reshape(-1, 1)\n",
        "scaler.fit(y)\n",
        "\n",
        "# Get last 1440 rows from test set\n",
        "last_1440 = X_test.shape[0] - 1440\n",
        "\n",
        "X_test_last_1440 = X_test[last_1440:]\n",
        "y_test_last_1440 = y_test[last_1440:]\n",
        "\n",
        "# Make predictions\n",
        "y_pred_last_1440 = model.predict(X_test_last_1440)\n",
        "\n",
        "# Inverse transform predictions\n",
        "y_pred_last_1440 = scaler.inverse_transform(y_pred_last_1440).flatten()\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = math.sqrt(mean_squared_error(y_test_last_1440, y_pred_last_1440))\n",
        "\n",
        "# Create DataFrame\n",
        "result_df = pd.DataFrame({\n",
        "    'Actual': y_test_last_1440,\n",
        "    'Predicted': y_pred_last_1440,\n",
        "    'RMSE': rmse\n",
        "})\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "# Create rows to write\n",
        "rows = [\n",
        "    [actual, predicted, rmse]\n",
        "    for actual, predicted in zip(y_test_last_1440, y_pred_last_1440)\n",
        "]\n",
        "\n",
        "\n",
        "# Open csv file for writing\n",
        "with open('predictions.csv', 'w') as f:\n",
        "\n",
        "    # Create csv writer\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # Write column headers\n",
        "    writer.writerow(['Actual', 'Predicted', 'RMSE'])\n",
        "\n",
        "    # Write each row\n",
        "    writer.writerows(rows)\n",
        "\n",
        "print('CSV saved successfully!')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTM7TAw88J3e",
        "outputId": "4cb6d440-6d1f-4f97-c963-84242cb78775"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 386928.2812 - mae: 358.5123 - val_loss: 159048.9531 - val_mae: 205.7483\n",
            "Epoch 2/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 169052.9688 - mae: 197.5963 - val_loss: 162422.6719 - val_mae: 157.3386\n",
            "Epoch 3/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 163661.4688 - mae: 180.8880 - val_loss: 150917.0781 - val_mae: 184.7164\n",
            "Epoch 4/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 161693.3281 - mae: 174.8872 - val_loss: 147493.2812 - val_mae: 164.4979\n",
            "Epoch 5/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 160761.6875 - mae: 174.0739 - val_loss: 150294.9844 - val_mae: 183.8889\n",
            "Epoch 6/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 160160.4531 - mae: 172.5674 - val_loss: 150268.3594 - val_mae: 171.1640\n",
            "Epoch 7/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 159240.1406 - mae: 171.2404 - val_loss: 150787.7500 - val_mae: 193.6811\n",
            "Epoch 8/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 159605.3125 - mae: 171.6427 - val_loss: 153761.6719 - val_mae: 203.9313\n",
            "Epoch 9/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 158092.8906 - mae: 169.2831 - val_loss: 151518.3906 - val_mae: 152.5148\n",
            "Epoch 10/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 159101.7656 - mae: 171.2715 - val_loss: 146429.2656 - val_mae: 162.6420\n",
            "Epoch 11/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 158865.0156 - mae: 170.7237 - val_loss: 151599.9219 - val_mae: 194.8008\n",
            "Epoch 12/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 158905.6562 - mae: 171.9269 - val_loss: 150029.2969 - val_mae: 147.7990\n",
            "Epoch 13/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 159088.0938 - mae: 170.4865 - val_loss: 147345.2188 - val_mae: 157.7596\n",
            "Epoch 14/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 157475.4062 - mae: 167.7365 - val_loss: 149251.3594 - val_mae: 185.9062\n",
            "Epoch 15/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 158400.2812 - mae: 169.4049 - val_loss: 150180.0000 - val_mae: 147.7738\n",
            "Epoch 16/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 158699.9844 - mae: 170.4175 - val_loss: 151353.7656 - val_mae: 198.1235\n",
            "Epoch 17/50\n",
            "948/948 [==============================] - 2s 3ms/step - loss: 157221.6562 - mae: 168.4066 - val_loss: 146093.0312 - val_mae: 156.0468\n",
            "Epoch 18/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 157267.2500 - mae: 167.4450 - val_loss: 151454.6875 - val_mae: 146.0689\n",
            "Epoch 19/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 156890.8281 - mae: 166.9632 - val_loss: 148458.4844 - val_mae: 180.6501\n",
            "Epoch 20/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 157469.2656 - mae: 168.5444 - val_loss: 147582.7031 - val_mae: 146.6788\n",
            "Epoch 21/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 157281.3125 - mae: 168.6335 - val_loss: 152995.1875 - val_mae: 199.4582\n",
            "Epoch 22/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 157079.8594 - mae: 167.8116 - val_loss: 147642.2188 - val_mae: 152.4226\n",
            "Epoch 23/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 156430.1562 - mae: 166.8814 - val_loss: 154488.6250 - val_mae: 202.4938\n",
            "Epoch 24/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 157296.2500 - mae: 167.7529 - val_loss: 146002.0625 - val_mae: 151.9951\n",
            "Epoch 25/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 157047.1250 - mae: 167.8924 - val_loss: 149550.5781 - val_mae: 144.5925\n",
            "Epoch 26/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 156952.6406 - mae: 166.2736 - val_loss: 151659.3750 - val_mae: 190.1088\n",
            "Epoch 27/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 155766.1875 - mae: 165.6010 - val_loss: 147774.5469 - val_mae: 152.5160\n",
            "Epoch 28/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 156185.6875 - mae: 165.1061 - val_loss: 147702.1250 - val_mae: 158.6314\n",
            "Epoch 29/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 155684.6094 - mae: 165.3677 - val_loss: 151640.6562 - val_mae: 158.3638\n",
            "Epoch 30/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 155840.1875 - mae: 165.3812 - val_loss: 147920.9375 - val_mae: 147.7781\n",
            "Epoch 31/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 156542.7812 - mae: 165.4916 - val_loss: 147191.6094 - val_mae: 148.8967\n",
            "Epoch 32/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 155994.7188 - mae: 165.2886 - val_loss: 146839.2344 - val_mae: 155.4227\n",
            "Epoch 33/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 156113.6562 - mae: 165.5784 - val_loss: 154812.1406 - val_mae: 166.7600\n",
            "Epoch 34/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 156032.9531 - mae: 165.2811 - val_loss: 148608.1250 - val_mae: 181.5806\n",
            "Epoch 35/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 155692.0312 - mae: 164.5092 - val_loss: 146388.2969 - val_mae: 152.4092\n",
            "Epoch 36/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 156025.9062 - mae: 164.7264 - val_loss: 148373.1406 - val_mae: 177.8797\n",
            "Epoch 37/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 156319.1094 - mae: 166.1944 - val_loss: 147515.0938 - val_mae: 153.6849\n",
            "Epoch 38/50\n",
            "948/948 [==============================] - 2s 3ms/step - loss: 155490.4062 - mae: 164.3020 - val_loss: 152632.7656 - val_mae: 196.4697\n",
            "Epoch 39/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 155999.9531 - mae: 164.4905 - val_loss: 149924.9531 - val_mae: 179.9622\n",
            "Epoch 40/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 155475.1562 - mae: 164.0137 - val_loss: 146002.2656 - val_mae: 161.4637\n",
            "Epoch 41/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 156006.9844 - mae: 165.5979 - val_loss: 147206.3438 - val_mae: 158.6148\n",
            "Epoch 42/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 155207.4531 - mae: 164.0490 - val_loss: 146233.5312 - val_mae: 146.6530\n",
            "Epoch 43/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 155571.5625 - mae: 165.0288 - val_loss: 151378.4219 - val_mae: 145.8686\n",
            "Epoch 44/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 155613.5938 - mae: 164.3455 - val_loss: 149317.3125 - val_mae: 173.5363\n",
            "Epoch 45/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 155217.0469 - mae: 163.5653 - val_loss: 146856.5625 - val_mae: 164.4852\n",
            "Epoch 46/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 155785.1406 - mae: 165.1904 - val_loss: 146938.0000 - val_mae: 166.4590\n",
            "Epoch 47/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 155809.5312 - mae: 164.3073 - val_loss: 145931.0938 - val_mae: 157.5157\n",
            "Epoch 48/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 156110.3281 - mae: 164.8430 - val_loss: 151877.2031 - val_mae: 190.7125\n",
            "Epoch 49/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 155121.9219 - mae: 164.4887 - val_loss: 150465.2969 - val_mae: 149.7489\n",
            "Epoch 50/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 156267.7500 - mae: 166.2858 - val_loss: 147108.2969 - val_mae: 150.4637\n",
            "316/316 [==============================] - 0s 1ms/step - loss: 157534.2969 - mae: 152.2066\n",
            "Test Mean Absolute Error: 152.20657348632812\n",
            "45/45 [==============================] - 0s 1ms/step\n",
            "            Actual     Predicted          RMSE\n",
            "35410   720.594482  2.178398e+06  6.668939e+06\n",
            "23250   632.309814  2.379462e+06  6.668939e+06\n",
            "35570   470.704193  1.712162e+06  6.668939e+06\n",
            "11715  1920.000000  6.645934e+06  6.668939e+06\n",
            "40256   556.119385  2.066000e+06  6.668939e+06\n",
            "...            ...           ...           ...\n",
            "2254   3414.837891  1.243454e+07  6.668939e+06\n",
            "6224   1328.156982  3.682776e+06  6.668939e+06\n",
            "33427   215.810501  8.227719e+05  6.668939e+06\n",
            "36504  3389.184082  1.181621e+07  6.668939e+06\n",
            "28445     0.000000  2.881901e+03  6.668939e+06\n",
            "\n",
            "[1440 rows x 3 columns]\n",
            "CSV saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Future Prediction - 24 hrs\n"
      ],
      "metadata": {
        "id": "NO0wg67XfS7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Train model only on training data\n",
        "model.fit(X_train, y_train, epochs=50)\n",
        "\n",
        "# No of features\n",
        "no_features = X_train.shape[1]\n",
        "\n",
        "# Create dummy 2D input\n",
        "next_1440 = pd.DataFrame(np.zeros((1440, no_features)))\n",
        "\n",
        "# Make predictions\n",
        "y_pred_next_1440 = model.predict(next_1440)\n",
        "\n",
        "# Inverse transform predictions\n",
        "y_pred_next_1440 = scaler.inverse_transform(y_pred_next_1440)\n",
        "\n",
        "# RMSE will be Nan as we don't have actual values\n",
        "rmse = [np.nan]*len(y_pred_next_1440)\n",
        "\n",
        "# Prepare rows for writing to CSV\n",
        "rows = [\n",
        "    [np.nan, pred, rmse_val]\n",
        "    for pred, rmse_val in zip(y_pred_next_1440, rmse)\n",
        "]\n",
        "\n",
        "# Write predictions to CSV\n",
        "with open('future_predictions.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Actual','Predicted','RMSE'])\n",
        "    writer.writerows(rows)\n",
        "\n",
        "print(rmse)\n",
        "\n",
        "print('Future predictions saved to CSV')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUyodNNUbq57",
        "outputId": "dfe998cf-6edc-44e8-e021-83f003b63023"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152794.4062 - mae: 162.0071\n",
            "Epoch 2/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152913.3125 - mae: 162.5797\n",
            "Epoch 3/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152868.5938 - mae: 162.3024\n",
            "Epoch 4/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152964.4375 - mae: 162.1060\n",
            "Epoch 5/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152757.5625 - mae: 162.7124\n",
            "Epoch 6/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152963.2500 - mae: 161.8400\n",
            "Epoch 7/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 153008.6094 - mae: 161.7057\n",
            "Epoch 8/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152908.5000 - mae: 162.6572\n",
            "Epoch 9/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152559.6250 - mae: 161.3618\n",
            "Epoch 10/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152996.9375 - mae: 162.3999\n",
            "Epoch 11/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152918.2188 - mae: 162.1423\n",
            "Epoch 12/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152739.6719 - mae: 162.0127\n",
            "Epoch 13/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152636.4844 - mae: 161.8892\n",
            "Epoch 14/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152744.8750 - mae: 162.0830\n",
            "Epoch 15/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152923.0625 - mae: 162.1781\n",
            "Epoch 16/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152908.4062 - mae: 162.2738\n",
            "Epoch 17/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152946.0938 - mae: 162.0924\n",
            "Epoch 18/50\n",
            "948/948 [==============================] - 2s 3ms/step - loss: 152670.0156 - mae: 162.5799\n",
            "Epoch 19/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 153037.7812 - mae: 161.5636\n",
            "Epoch 20/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 153008.5625 - mae: 162.5582\n",
            "Epoch 21/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152689.5000 - mae: 161.8817\n",
            "Epoch 22/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152878.2656 - mae: 161.8936\n",
            "Epoch 23/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152951.1406 - mae: 162.7070\n",
            "Epoch 24/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152838.0938 - mae: 161.7722\n",
            "Epoch 25/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152906.2500 - mae: 161.9310\n",
            "Epoch 26/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152932.1875 - mae: 161.8447\n",
            "Epoch 27/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152903.9531 - mae: 162.1850\n",
            "Epoch 28/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152876.2188 - mae: 161.6846\n",
            "Epoch 29/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152566.5938 - mae: 161.6965\n",
            "Epoch 30/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152771.0312 - mae: 162.2166\n",
            "Epoch 31/50\n",
            "948/948 [==============================] - 2s 3ms/step - loss: 153010.4062 - mae: 162.7346\n",
            "Epoch 32/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152782.8906 - mae: 161.8095\n",
            "Epoch 33/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152799.7031 - mae: 162.2354\n",
            "Epoch 34/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152897.9062 - mae: 162.1070\n",
            "Epoch 35/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152783.0625 - mae: 162.4368\n",
            "Epoch 36/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152567.1875 - mae: 161.4799\n",
            "Epoch 37/50\n",
            "948/948 [==============================] - 3s 3ms/step - loss: 152800.6562 - mae: 161.8875\n",
            "Epoch 38/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152791.5938 - mae: 161.4973\n",
            "Epoch 39/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152964.2969 - mae: 162.5575\n",
            "Epoch 40/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152956.5000 - mae: 162.0056\n",
            "Epoch 41/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152786.5312 - mae: 162.4332\n",
            "Epoch 42/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152829.8438 - mae: 162.0446\n",
            "Epoch 43/50\n",
            "948/948 [==============================] - 2s 3ms/step - loss: 153039.1875 - mae: 161.9814\n",
            "Epoch 44/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152648.7344 - mae: 161.3146\n",
            "Epoch 45/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152646.2031 - mae: 161.8010\n",
            "Epoch 46/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 153158.4062 - mae: 162.5629\n",
            "Epoch 47/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 153060.6250 - mae: 161.9228\n",
            "Epoch 48/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152677.3750 - mae: 161.1163\n",
            "Epoch 49/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152651.3594 - mae: 161.9837\n",
            "Epoch 50/50\n",
            "948/948 [==============================] - 2s 2ms/step - loss: 152710.7656 - mae: 162.2879\n",
            "45/45 [==============================] - 0s 1ms/step\n",
            "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
            "Future predictions saved to CSV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please use following link to see output files and resources - https://drive.google.com/drive/folders/1P3PMPAP9C5IkDpmq3iyoC6WPdJL2SYMV?usp=sharing"
      ],
      "metadata": {
        "id": "QzCNLmwnm_4w"
      }
    }
  ]
}